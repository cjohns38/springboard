{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle, gensim, logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords \n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import the data \n",
    "\n",
    "data = pickle.load(open('onetsoccode.p', 'rb'))\n",
    "jobtitles = pickle.load(open('lookuptable', 'rb'))\n",
    "\n",
    "# Stop words \n",
    "stopWords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to DF\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df2 = pd.DataFrame(jobtitles).T\n",
    "\n",
    "data = pd.merge(df, \n",
    "                  df2, \n",
    "                  left_on = 'soc', \n",
    "                  right_on = 'soc', \n",
    "                  how = 'left'\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to preprocess the text \n",
    "\n",
    "def preprocessText(txt, stopwords): \n",
    "    \"\"\"Use gensim simple preprocess and remove stopwords\"\"\"\n",
    "    stopwords = stopwords\n",
    "    txt = gensim.utils.simple_preprocess(txt)\n",
    "    return  [word for word in txt if word not in stopwords]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess\n",
    "\n",
    "data['processed'] = data.apply(lambda x: preprocessText(x['text'], stopWords), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import modules & set up logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 07:22:09,125 : INFO : loading projection weights from /home/craig/Documents/googledata/GoogleNews-vectors-negative300.bin\n",
      "2017-11-04 07:22:59,735 : INFO : loaded (3000000, 300) matrix from /home/craig/Documents/googledata/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "# Pretrained vectors from google\n",
    "### https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format('/home/craig/Documents/googledata/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a numpy array of the model word_vec \n",
    "\n",
    "def average_word_embedding(data, model, keyed = False): \n",
    "    \"\"\"Return the average word embedding\"\"\"\n",
    "    out = []\n",
    "    \n",
    "    # Handle keyed vectors \n",
    "    if keyed == True: \n",
    "        find_vector = lambda word: model.word_vec(word)\n",
    "    else: \n",
    "        find_vector = lambda word: model.wv[word]\n",
    "        \n",
    "    # Loop through \n",
    "    for word in data: \n",
    "        if word in model:\n",
    "            out.append(find_vector(word))\n",
    "    \n",
    "    # calculate average \n",
    "    avg = np.average(np.array(out), axis = 0)\n",
    "    return pd.Series({'d' + str(idx):x for idx, x in enumerate(avg)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the avg word vectors \n",
    "\n",
    "variables = ['d' + str(x) for x in range(0,300)]\n",
    "data[variables] = data.apply(lambda x: average_word_embedding(x['processed'], word_vectors, True), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a function that finds the best "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(target_vector, row): \n",
    "    job1 = target_vector\n",
    "    job2 = row.loc['d0':].values\n",
    "    numerator = np.dot(job1,job2)\n",
    "    denominator = np.sqrt(np.sum(job1**2)) * np.sqrt(np.sum(job2**2))\n",
    "    return pd.Series([numerator/denominator])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_jobs(data, soc, topN, bottomN): \n",
    "    \"\"\" Given the ONET job DF and a SOC code find the top and bottom similar jobs \"\"\"\n",
    "    \n",
    "    # Create a DF of the target and non-target jobs\n",
    "    target = data[data['soc'] == soc]\n",
    "    df = data[data['soc'] != soc]\n",
    "\n",
    "    # Create target vector \n",
    "    target_vector = data[data['soc'] == soc].loc[:, 'd0':].values[0]\n",
    "    \n",
    "    # Run similarities \n",
    "    s = df.apply(lambda x: cosine_similarity(target_vector, x), axis = 1)\n",
    "    df = df.assign(similarity = s)\n",
    "    \n",
    "    # Sort the values \n",
    "    df.sort_values(by = 'similarity', ascending = False, inplace = True)\n",
    "    \n",
    "    # Print the top N \n",
    "    top = df[['title', 'similarity']].head(topN).values.tolist()\n",
    "    \n",
    "    # Print the bottom N \n",
    "    bottom = df[['title', 'similarity']].tail(bottomN).values.tolist()\n",
    "    \n",
    "    # Print results \n",
    "    print(\"For the job of {0}...\".format(target['title'].values[0]))\n",
    "    \n",
    "    # Top \n",
    "    print(\"The most similiar jobs are...\".format(target['title'].values[0]))\n",
    "    for job in top: \n",
    "        print(\"\\t {0}\".format(job[0]))\n",
    "    \n",
    "    # Bottom \n",
    "    print(\"The least similar jobs are...\")\n",
    "    for job in bottom: \n",
    "        print('\\t {0}'.format(job[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Singers...\n",
      "The most similiar jobs are...\n",
      "\t Music Directors\n",
      "\t Choreographers\n",
      "\t Public Address System and Other Announcers\n",
      "\t Actors\n",
      "\t Talent Directors\n",
      "The least similar jobs are...\n",
      "\t Methane/Landfill Gas Generation System Technicians\n",
      "\t Green Marketers\n",
      "\t Methane/Landfill Gas Collection System Operators\n",
      "\t Fuel Cell Technicians\n",
      "\t Data Warehousing Specialists\n"
     ]
    }
   ],
   "source": [
    "# Lets look at singers\n",
    "find_jobs(data, '27-2042.01', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Automotive Master Mechanics...\n",
      "The most similiar jobs are...\n",
      "\t Bus and Truck Mechanics and Diesel Engine Specialists\n",
      "\t Automotive Specialty Technicians\n",
      "\t Mobile Heavy Equipment Mechanics, Except Engines\n",
      "\t Outdoor Power Equipment and Other Small Engine Mechanics\n",
      "\t Recreational Vehicle Service Technicians\n",
      "The least similar jobs are...\n",
      "\t Software Developers, Applications\n",
      "\t Green Marketers\n",
      "\t Data Warehousing Specialists\n",
      "\t Investment Underwriters\n",
      "\t Legislators\n"
     ]
    }
   ],
   "source": [
    "# Now lets look at a mechanical job \n",
    "find_jobs(data, '49-3023.01', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Industrial-Organizational Psychologists...\n",
      "The most similiar jobs are...\n",
      "\t Natural Sciences Managers\n",
      "\t Sociologists\n",
      "\t Logisticians\n",
      "\t Social and Community Service Managers\n",
      "\t Counseling Psychologists\n",
      "The least similar jobs are...\n",
      "\t Landscaping and Groundskeeping Workers\n",
      "\t Bakers\n",
      "\t Musical Instrument Repairers and Tuners\n",
      "\t Helpers--Pipelayers, Plumbers, Pipefitters, and Steamfitters\n",
      "\t Agricultural Equipment Operators\n"
     ]
    }
   ],
   "source": [
    "# Now lets look at I/O Psychologist \n",
    "find_jobs(data, '19-3032.00', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Computer Programmers...\n",
      "The most similiar jobs are...\n",
      "\t Software Developers, Applications\n",
      "\t Software Developers, Systems Software\n",
      "\t Software Quality Assurance Engineers and Testers\n",
      "\t Computer Systems Engineers/Architects\n",
      "\t Database Administrators\n",
      "The least similar jobs are...\n",
      "\t Landscaping and Groundskeeping Workers\n",
      "\t Surgeons\n",
      "\t Hunters and Trappers\n",
      "\t Helpers--Pipelayers, Plumbers, Pipefitters, and Steamfitters\n",
      "\t Agricultural Equipment Operators\n"
     ]
    }
   ],
   "source": [
    "# Now lets look at Computer programmers\n",
    "find_jobs(data, '15-1131.00', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Surgeons...\n",
      "The most similiar jobs are...\n",
      "\t Surgical Technologists\n",
      "\t Oral and Maxillofacial Surgeons\n",
      "\t Obstetricians and Gynecologists\n",
      "\t Surgical Assistants\n",
      "\t Veterinarians\n",
      "The least similar jobs are...\n",
      "\t Software Developers, Applications\n",
      "\t Green Marketers\n",
      "\t Investment Underwriters\n",
      "\t Legislators\n",
      "\t Data Warehousing Specialists\n"
     ]
    }
   ],
   "source": [
    "# Now lets look at Computer programmers\n",
    "find_jobs(data, '29-1067.00', 5, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roll my own model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-04 07:36:54,181 : INFO : collecting all words and their counts\n",
      "2017-11-04 07:36:54,183 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-11-04 07:36:54,350 : INFO : collected 24588 word types from a corpus of 666976 raw words and 974 sentences\n",
      "2017-11-04 07:36:54,351 : INFO : Loading a fresh vocabulary\n",
      "2017-11-04 07:36:54,378 : INFO : min_count=5 retains 7523 unique words (30% of original 24588, drops 17065)\n",
      "2017-11-04 07:36:54,380 : INFO : min_count=5 leaves 639263 word corpus (95% of original 666976, drops 27713)\n",
      "2017-11-04 07:36:54,410 : INFO : deleting the raw counts dictionary of 24588 items\n",
      "2017-11-04 07:36:54,414 : INFO : sample=0.001 downsamples 46 most-common words\n",
      "2017-11-04 07:36:54,414 : INFO : downsampling leaves estimated 584754 word corpus (91.5% of prior 639263)\n",
      "2017-11-04 07:36:54,415 : INFO : estimated required memory for 7523 words and 300 dimensions: 21816700 bytes\n",
      "2017-11-04 07:36:54,437 : INFO : resetting layer weights\n",
      "2017-11-04 07:36:54,561 : INFO : training model with 4 workers on 7523 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-11-04 07:36:55,567 : INFO : PROGRESS: at 13.70% examples, 1198614 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 07:36:56,572 : INFO : PROGRESS: at 27.45% examples, 1202277 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 07:36:57,573 : INFO : PROGRESS: at 40.90% examples, 1193960 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 07:36:58,574 : INFO : PROGRESS: at 54.44% examples, 1191923 words/s, in_qsize 8, out_qsize 1\n",
      "2017-11-04 07:36:59,583 : INFO : PROGRESS: at 67.68% examples, 1183634 words/s, in_qsize 8, out_qsize 0\n",
      "2017-11-04 07:37:00,590 : INFO : PROGRESS: at 80.53% examples, 1172967 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 07:37:01,593 : INFO : PROGRESS: at 92.76% examples, 1157603 words/s, in_qsize 7, out_qsize 0\n",
      "2017-11-04 07:37:02,158 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-11-04 07:37:02,163 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-11-04 07:37:02,168 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-11-04 07:37:02,180 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-11-04 07:37:02,183 : INFO : training on 10004640 raw words (8772401 effective words) took 7.6s, 1151525 effective words/s\n"
     ]
    }
   ],
   "source": [
    "mymodel = gensim.models.Word2Vec(data.processed, size=300, window = 5, iter=15, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = data.loc[:, :'processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['d' + str(x) for x in range(0,300)]\n",
    "model2[variables] = model2.apply(lambda x: average_word_embedding(x['processed'], mymodel), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the job of Surgeons...\n",
      "The most similiar jobs are...\n",
      "\t Veterinarians\n",
      "\t Oral and Maxillofacial Surgeons\n",
      "\t Surgical Technologists\n",
      "\t Veterinary Technologists and Technicians\n",
      "\t Acute Care Nurses\n",
      "The least similar jobs are...\n",
      "\t Methane/Landfill Gas Collection System Operators\n",
      "\t Fuel Cell Technicians\n",
      "\t Investment Underwriters\n",
      "\t Data Warehousing Specialists\n",
      "\t Green Marketers\n"
     ]
    }
   ],
   "source": [
    "find_jobs(model2, '29-1067.00', 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
